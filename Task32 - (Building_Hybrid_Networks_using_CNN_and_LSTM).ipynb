{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "####**Task32 - (Building Hybrid Networks using CNN and LSTM)**\n",
        "\n",
        "Hybrid models refer to a combination of different models used together to solve a specific problem. These models integrate multiple techniques to leverage the strengths of each component and enhance overall performance.\n",
        "\n",
        "In the context of machine learning and artificial intelligence, hybrid models often involve the fusion of different types of models, such as combining traditional statistical models with neural networks or combining rule-based systems with machine learning algorithms. The goal is to create a more powerful and robust model that can effectively handle complex tasks and exploit various types of data."
      ],
      "metadata": {
        "id": "hKVNBiOH3hti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sentiment Analysis with IMDB Movie Reviews Dataset:**"
      ],
      "metadata": {
        "id": "2btVac2XpRk0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "rZ8S5nQ1atef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "421867c8-88cb-4175-950e-83590876b0c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 1s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 500)\n",
            "x_test shape: (25000, 500)\n"
          ]
        }
      ],
      "source": [
        "#Listing 6.45 Preparing the IMDB data\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Conv1D, MaxPooling1D, LSTM\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "max_features = 10000\n",
        "max_len = 500\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100  # Dimensionality of word embeddings\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_dim, input_length=max_len))\n",
        "model.add(Conv1D(128, 5, activation='relu'))\n",
        "model.add(MaxPooling1D(5))\n",
        "model.add(LSTM(128))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_test, y_test))\n",
        "\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jaBI85YRp2GE",
        "outputId": "686409e6-bfb8-4f64-80a3-cabfb6b5c0c9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 116s 132ms/step - loss: 0.3439 - accuracy: 0.8465 - val_loss: 0.2763 - val_accuracy: 0.8875\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 40s 52ms/step - loss: 0.1793 - accuracy: 0.9344 - val_loss: 0.3152 - val_accuracy: 0.8696\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.1076 - accuracy: 0.9640 - val_loss: 0.3650 - val_accuracy: 0.8720\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 18s 23ms/step - loss: 0.0621 - accuracy: 0.9803 - val_loss: 0.4385 - val_accuracy: 0.8714\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 19s 24ms/step - loss: 0.0367 - accuracy: 0.9889 - val_loss: 0.4845 - val_accuracy: 0.8696\n",
            "782/782 [==============================] - 4s 4ms/step - loss: 0.4845 - accuracy: 0.8696\n",
            "Test Loss: 0.4845\n",
            "Test Accuracy: 0.8696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We can use the hybrid models to perform Image recognition:**"
      ],
      "metadata": {
        "id": "RMj75K7TKw_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, LSTM, Reshape\n",
        "from keras.datasets import cifar10\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "cifar10 = keras.datasets.cifar10\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values between 0 and 1\n",
        "train_images = train_images.astype('float32') / 255.0\n",
        "test_images = test_images.astype('float32') / 255.0\n",
        "\n",
        "#Perform one-hot encoding on the train and test labels\n",
        "train_labels = to_categorical(train_labels, 10)         # No. of classes = 10\n",
        "test_labels = to_categorical(test_labels, 10)"
      ],
      "metadata": {
        "id": "tdYj7YIPKqae"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))    # Performs max pooling with a pool size of (2, 2) to downsample the feature maps.\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same')) #Additional Conv2D and MaxPooling2D layers follow, increasing the complexity of the learned features.\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model.add(Flatten())         # Converts the 2D feature maps into a 1D feature vector.\n",
        "model.add(Reshape((1, -1)))  # Reshape the feature vector to match LSTM input, with a batch size of 1 and unknown sequence length (-1).\n",
        "model.add(LSTM(128))         # Processes the sequential information encoded in the reshaped feature vector\n",
        "model.add(Dense(64, activation='relu'))    # Fully connected layers that perform classification on the output of the LSTM layer.\n",
        "model.add(Dense(10, activation='softmax'))  # Final dense layer with 10 units and 'softmax' activation, which outputs probabilities for the 10 classes.\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_images, train_labels, epochs=10, batch_size=32, validation_data=(test_images, test_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nem3-qqxfKup",
        "outputId": "50003a53-cf76-4bab-8a57-0083aa89c357"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 17s 8ms/step - loss: 1.4090 - accuracy: 0.4902 - val_loss: 1.1061 - val_accuracy: 0.6115\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 12s 7ms/step - loss: 0.9447 - accuracy: 0.6674 - val_loss: 0.9491 - val_accuracy: 0.6666\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.7212 - accuracy: 0.7481 - val_loss: 0.8527 - val_accuracy: 0.7026\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.5434 - accuracy: 0.8092 - val_loss: 0.9250 - val_accuracy: 0.6993\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 12s 8ms/step - loss: 0.3817 - accuracy: 0.8678 - val_loss: 0.9381 - val_accuracy: 0.7145\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 13s 8ms/step - loss: 0.2482 - accuracy: 0.9148 - val_loss: 1.0238 - val_accuracy: 0.7137\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1717 - accuracy: 0.9401 - val_loss: 1.1798 - val_accuracy: 0.7083\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1332 - accuracy: 0.9537 - val_loss: 1.2714 - val_accuracy: 0.7060\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1186 - accuracy: 0.9577 - val_loss: 1.4075 - val_accuracy: 0.6978\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 11s 7ms/step - loss: 0.1055 - accuracy: 0.9627 - val_loss: 1.5630 - val_accuracy: 0.6843\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd27d6d57b0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tocnMOIle7vs",
        "outputId": "71882470-3ae4-4091-d3c4-7da8b350275e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 4ms/step - loss: 1.5630 - accuracy: 0.6843\n",
            "Test Loss: 1.5630\n",
            "Test Accuracy: 0.6843\n"
          ]
        }
      ]
    }
  ]
}