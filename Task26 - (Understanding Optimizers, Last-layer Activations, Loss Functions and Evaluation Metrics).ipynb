{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5bc6e79",
   "metadata": {},
   "source": [
    "# Understading Optimizers\n",
    "Optimizers are algorithms used to update the parameters (weights and biases) of a neural network during training to minimize the loss function. They work by adjusting the parameters in the direction of the steepest descent of the loss function. Some popular optimizers include stochastic gradient descent (SGD), Adam, RMSprop, and Adagrad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0334d63",
   "metadata": {},
   "source": [
    "# Last-layer activations\n",
    "The last-layer activation function is used to transform the output of the last layer of a neural network into a format suitable for the problem at hand. For example, in a binary classification problem, the last layer activation function is often a sigmoid function that outputs a probability value between 0 and 1. In a multi-class classification problem, the last layer activation function is often a softmax function that outputs a probability distribution over all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea44365",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "A loss function is used to measure how well the model's predictions match the actual values in the training data. The optimizer then uses the loss function to adjust the parameters of the model to minimize the loss. Some commonly used loss functions include mean squared error (MSE), binary cross-entropy, and categorical cross-entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaba56e",
   "metadata": {},
   "source": [
    "# Evaluation metrics\n",
    "Evaluation metrics are used to measure the performance of the model on the test data. Some commonly used evaluation metrics for classification problems include accuracy, precision, recall, and F1-score. For regression problems, commonly used evaluation metrics include mean squared error (MSE), root mean squared error (RMSE), and mean absolute error (MAE).Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d67dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definition\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape=(784,)))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b68c7",
   "metadata": {},
   "source": [
    "The last layer uses a softmax activation. You saw this pattern in the MNIST example. It means the network will output a probability distribution over the 10 different output classesâ€”for every input sample, the network will produce a 10-dimensional output vector, where output[i] is the probability that the sample belongs to class i. The 10 scores will sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model\n",
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ab8d9",
   "metadata": {},
   "source": [
    "An instance of the **RMSprop optimizer** is created with a learning rate of 0.001. The learning rate determines the step size at each iteration while moving toward a minimum of a loss function during training.\n",
    "\n",
    "The loss function is specified as **categorical_crossentropy**, which is commonly used as the loss function for multi-class classification problems.\n",
    "\n",
    "The code **metrics=['accuracy']** specifies the evaluation metric used to measure the performance of the model. In this case, accuracy is used, which is the fraction of correctly classified samples over the total number of samples. Other metrics include precision, recall, F1-score, and custom metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
