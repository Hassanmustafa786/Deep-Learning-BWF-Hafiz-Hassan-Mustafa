{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7eee394",
   "metadata": {},
   "source": [
    "# Evaluating machine-learning models\n",
    "The chapter discusses the importance of splitting data into a training set, validation set, and test set in order to measure the generalization power of machine learning models. Overfitting is a common problem that occurs when a model performs well on training data but poorly on unseen data. The chapter emphasizes the importance of evaluating models on never-before-seen data to achieve models that generalize well. Strategies for mitigating overfitting and maximizing generalization are also discussed. The section focuses on how to measure generalization and how to evaluate machine-learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be8327f",
   "metadata": {},
   "source": [
    "# Training, validation, and test sets\n",
    "To evaluate a machine learning model, the available data is split into three sets: \n",
    "1. **Training.**\n",
    "2. **Validation.**\n",
    "3. **Test.**\n",
    "\n",
    "The model is trained on the training data, and its performance is evaluated on the validation data, which is used to tune the model's configuration. However, this can result in overfitting to the validation set, and information leaks can occur, causing the model to perform artificially well on the validation data. Therefore, a completely different, never-before-seen dataset (the test dataset) should be used to evaluate the model's generalization. Advanced methods of splitting data into training, validation, and test sets, such as simple **hold-out validation**, **K-fold validation**, and **iterated K-fold validation with shuffling**, can be used when little data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8affc8c4",
   "metadata": {},
   "source": [
    "# Simple Hold-Out Validation\n",
    "Set apart some fraction of your data as your test set. Train on the remaining data, and evaluate on the test set. As you saw in the previous sections, in order to prevent information leaks, you shouldn’t tune your model based on the test set, and therefore you should also reserve a validation set.\n",
    "Schematically, hold-out validation looks like figure. The following listing shows a simple implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d93e9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://image.ibb.co/f6f1my/hold_out_validation.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://image.ibb.co/f6f1my/hold_out_validation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9f9e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hold-out validation code...\n",
    "num_validation_samples = 10000\n",
    "\n",
    "# Shuffling the data is usually appropriate.\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Defines the validation set\n",
    "validation_data = data[:num_validation_samples]\n",
    "data = data[num_validation_samples:]\n",
    "\n",
    "# Defines the training set\n",
    "training_data = data[:]\n",
    "\n",
    "# Trains a model on the training data, and evaluates it on the validation data\n",
    "model = get_model()\n",
    "model.train(training_data)\n",
    "validation_score = model.evaluate(validation_data)\n",
    "\n",
    "# At this point you can tune your model,\n",
    "# retrain it, evaluate it, tune it again...\n",
    "model = get_model()\n",
    "model.train(np.concatenate([training_data,\n",
    "validation_data]))\n",
    "test_score = model.evaluate(test_data)\n",
    "\n",
    "# Once you’ve tuned your hyperparameters, it’s common to train your final model from scratch on all non-test data available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1d2293",
   "metadata": {},
   "source": [
    "This code splits the dataset data into training, validation, and test sets. The number of validation samples is set to num_validation_samples, which is 10000 in this case. Then, the data is shuffled randomly, and the first num_validation_samples are selected for the validation set, while the rest is used for the training set. A neural network model is created using get_model(), and it is trained on the training set. The validation set is used to evaluate the model's performance and obtain the validation_score. After tuning the model based on the validation score, the model is reinitialized using get_model(), and it is trained on the concatenated training and validation sets. Finally, the test set is used to evaluate the model's performance and obtain the test_score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d699212",
   "metadata": {},
   "source": [
    "# K-Fold Validation\n",
    "With this approach, you split your data into K partitions of equal size. For each partition i, train a model on the remaining\n",
    "K – 1 partitions, and evaluate it on partition i. Your final score is then the averages of the K scores obtained. This method is helpful when the performance of your model shows significant variance based on your train test split. Like hold-out validation, this method doesn’t exempt you from using a distinct validation set for model calibration.\n",
    "Schematically, K-fold cross-validation looks like figure. Listing shows a simple implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b4fa4d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://image.ibb.co/bUm7Ry/k_fold_validation.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://image.ibb.co/bUm7Ry/k_fold_validation.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ca723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Validation Code...\n",
    "k=4\n",
    "num_validation_samples = len(data) // k\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "validation_scores = []\n",
    "for fold in range(k):\n",
    "    # Selects the validation data partition\n",
    "    validation_data = data[num_validation_samples * fold:\n",
    "                           num_validation_samples * (fold + 1)]\n",
    "    \n",
    "    # Uses the remainder of the data as training data. Note that the + operator is list concatenation, not summation.\n",
    "    training_data = data[:num_validation_samples * fold] + data[num_validation_samples * (fold + 1):]\n",
    "    \n",
    "    # Creates a brand-new instance of the model (untrained)\n",
    "    model = get_model()\n",
    "    model.train(training_data)\n",
    "    validation_score = model.evaluate(validation_data)\n",
    "    validation_scores.append(validation_score)\n",
    "\n",
    "# Validation score: average of the validation scores of the k folds\n",
    "validation_score = np.average(validation_scores)\n",
    "\n",
    "# Trains the final model on all non-test data available\n",
    "model = get_model()\n",
    "model.train(data)\n",
    "test_score = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb82e98",
   "metadata": {},
   "source": [
    "This code performs k-fold cross-validation to evaluate a machine learning model. It first sets k to 4 and calculates the number of validation samples (num_validation_samples) by dividing the length of the data by k. Then, it shuffles the data randomly using np.random.shuffle().\n",
    "\n",
    "Next, the code initializes an empty list validation_scores to store the validation scores obtained for each fold. The for loop runs for k iterations, where in each iteration it selects a subset of the data as the validation data, and the rest of the data is used as training data. The get_model() function is called to initialize a new model for each iteration, and the model is trained on the training data using model.train(). The validation score is then calculated using model.evaluate() on the validation data, and the score is appended to the validation_scores list.\n",
    "\n",
    "This code performs k-fold cross-validation to evaluate a machine learning model. It first sets k to 4 and calculates the number of validation samples (num_validation_samples) by dividing the length of the data by k. Then, it shuffles the data randomly using np.random.shuffle().\n",
    "\n",
    "Next, the code initializes an empty list validation_scores to store the validation scores obtained for each fold. The for loop runs for k iterations, where in each iteration it selects a subset of the data as the validation data, and the rest of the data is used as training data. The get_model() function is called to initialize a new model for each iteration, and the model is trained on the training data using model.train(). The validation score is then calculated using model.evaluate() on the validation data, and the score is appended to the validation_scores list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee5ec95",
   "metadata": {},
   "source": [
    "# Iterated K-Fold Validation With Shuffling\n",
    "This one is for situations in which you have relatively little data available and you need to evaluate your model as precisely as possible. I’ve found it to be extremely helpful in Kaggle competitions. It consists of applying K-fold validation multiple times, shuffling the data every time before splitting it K ways. The final score is the average of the scores obtained at each run of K-fold validation. Note that you end up training and evaluating P × K models (where P is the number of iterations you use), which can very expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e744f",
   "metadata": {},
   "source": [
    "# Things to Keep in Mind\n",
    "1. **Data Representatives:** To ensure accurate training and testing in machine learning, it is important to have representative data sets. For example, if you are classifying images of digits, splitting your data set in a way that one set contains only certain classes and the other set contains different classes can lead to errors. Therefore, it is recommended to randomly shuffle your data before splitting it into training and testing sets.\n",
    "2. **The arrow of time:** When predicting the future based on past data, shuffling the data before splitting it into training and testing sets can cause a \"temporal leak\" by introducing future data into the training set. To avoid this, it's important to ensure that all the data in the test set is from a later time period than the data in the training set.\n",
    "3. **Redundancy in your data:** Duplicate data points are common in real-world data. If you shuffle such data and split it into training and validation sets, there may be overlap between the two sets, leading to testing on part of the training data, which is undesirable. To avoid this, make sure that the training and validation sets do not overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe181bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
